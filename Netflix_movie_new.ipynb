{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Netflix Movie Recommendation\n",
    "~Onkar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('nbagg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "import os\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random\n",
    "import statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statment:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References used:\n",
    "https://www.netflixprize.com/rules.html\n",
    "\n",
    "https://www.kaggle.com/netflix-inc/netflix-prize-data\n",
    "\n",
    "Netflix blog: https://medium.com/netflix-techblog/netflix-recommendations-beyond-the-5-stars-part-1-55838468f429\n",
    "\n",
    "surprise library: http://surpriselib.com/ (Used to build Baseline Models)\n",
    "\n",
    " http://surprise.readthedocs.io/en/stable/getting_started.html\n",
    "\n",
    "\n",
    "Research paper: http://courses.ischool.berkeley.edu/i290-dm/s11/SECURE/a1-koren.pdf (work was inspired by this paper)\n",
    "\n",
    "SVD Decomposition : https://www.youtube.com/watch?v=P5mlg91as1c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ratings from data_folder/combined_data_1.txt...\n",
      "Done.\n",
      "\n",
      "Reading ratings from data_folder/combined_data_2.txt...\n",
      "Done.\n",
      "\n",
      "Reading ratings from data_folder/combined_data_3.txt...\n",
      "Done.\n",
      "\n",
      "Reading ratings from data_folder/combined_data_4.txt...\n",
      "Done.\n",
      "\n",
      "Time taken : 0:07:27.751855\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "if not os.path.isfile('data.csv'):\n",
    "    # Create a file 'data.csv' before reading it\n",
    "    # Read all the files in netflix and store them in one big file('data.csv')\n",
    "    # We re reading from each of the four files and appendig each rating to a global file 'train.csv'\n",
    "    data = open('data.csv', mode='w')\n",
    "    \n",
    "    row = list()\n",
    "    files=['data_folder/combined_data_1.txt','data_folder/combined_data_2.txt', \n",
    "           'data_folder/combined_data_3.txt', 'data_folder/combined_data_4.txt']\n",
    "    for file in files:\n",
    "        #print(\"entered \")\n",
    "        print(\"Reading ratings from {}...\".format(file))\n",
    "        with open(file) as f:\n",
    "            for line in f: \n",
    "                del row[:] # you don't have to do this.\n",
    "                line = line.strip()\n",
    "                if line.endswith(':'):\n",
    "                    # All below are ratings for this movie, until another movie appears.\n",
    "                    movie_id = line.replace(':', '')\n",
    "                else:\n",
    "                    row = [x for x in line.split(',')]\n",
    "                    row.insert(0, movie_id)\n",
    "                    data.write(','.join(row))\n",
    "                    data.write('\\n')\n",
    "        print(\"Done.\\n\")\n",
    "    data.close()\n",
    "print('Time taken :', datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate array with shape (3, 100480507) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-9a184b4eb3f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'movie'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'user'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rating'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#arranging the ratings according to time.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\onkar\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\onkar\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\onkar\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1167\u001b[0m             \u001b[0mnew_rows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1169\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1171\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_currow\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnew_rows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\onkar\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    409\u001b[0m             )\n\u001b[0;32m    410\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\onkar\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[1;34m(data, index, columns, dtype)\u001b[0m\n\u001b[0;32m    255\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         ]\n\u001b[1;32m--> 257\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\onkar\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[0maxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\onkar\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_arrays\u001b[1;34m(arrays, names, axes)\u001b[0m\n\u001b[0;32m   1692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1693\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1694\u001b[1;33m         \u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mform_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1695\u001b[0m         \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBlockManager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1696\u001b[0m         \u001b[0mmgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\onkar\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mform_blocks\u001b[1;34m(arrays, names, axes)\u001b[0m\n\u001b[0;32m   1762\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1763\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitems_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"IntBlock\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1764\u001b[1;33m         \u001b[0mint_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_multi_blockify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitems_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"IntBlock\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1765\u001b[0m         \u001b[0mblocks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1766\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\onkar\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_multi_blockify\u001b[1;34m(tuples, dtype)\u001b[0m\n\u001b[0;32m   1844\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtup_block\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrouper\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1846\u001b[1;33m         \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplacement\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_stack_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup_block\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1847\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1848\u001b[0m         \u001b[0mblock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\onkar\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_stack_arrays\u001b[1;34m(tuples, dtype)\u001b[0m\n\u001b[0;32m   1872\u001b[0m     \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0m_shape_compat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1873\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1874\u001b[1;33m     \u001b[0mstacked\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1875\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1876\u001b[0m         \u001b[0mstacked\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_asarray_compat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate array with shape (3, 100480507) and data type int64"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv', sep=',', names=['movie', 'user','rating','date'])\n",
    "df.date = pd.to_datetime(df.date)\n",
    "#arranging the ratings according to time.\n",
    "df.sort_values(by='date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"No of Nan values in our dataframe : \", sum(df.isnull().any())) \n",
    "#all Nan containing needs to be deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for duplicate\n",
    "dup_bool = df.duplicated(['movie','user','rating'])\n",
    "dups = sum(dup_bool) \n",
    "print(\"There are {} duplicate rating entries in the data..\".format(dups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total data \")\n",
    "print(\"-\"*50)\n",
    "print(\"\\nTotal no of ratings :\",df.shape[0])\n",
    "print(\"Total No of Users   :\", len(np.unique(df.user)))\n",
    "print(\"Total No of movies  :\", len(np.unique(df.movie)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('train.csv'):\n",
    "    # create the dataframe and store it in the disk for offline purposes..\n",
    "    df.iloc[:int(df.shape[0]*0.80)].to_csv(\"train.csv\", index=False)\n",
    "\n",
    "if not os.path.isfile('test.csv'):\n",
    "    # create the dataframe and store it in the disk for offline purposes..\n",
    "    df.iloc[int(df.shape[0]*0.80):].to_csv(\"test.csv\", index=False)\n",
    "\n",
    "train_df = pd.read_csv(\"train.csv\", parse_dates=['date'])\n",
    "test_df = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movies = train_df.movie.value_counts()\n",
    "# users = train_df.user.value_counts()\n",
    "print(\"Training data \")\n",
    "print(\"-\"*50)\n",
    "print(\"\\nTotal no of ratings :\",train_df.shape[0])\n",
    "print(\"Total No of Users   :\", len(np.unique(train_df.user)))\n",
    "print(\"Total No of movies  :\", len(np.unique(train_df.movie)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test data \")\n",
    "print(\"-\"*50)\n",
    "print(\"\\nTotal no of ratings :\",test_df.shape[0])\n",
    "print(\"Total No of Users   :\", len(np.unique(test_df.user)))\n",
    "print(\"Total No of movies  :\", len(np.unique(test_df.movie)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to make y-axis more readable\n",
    "def human(num, units = 'M'):\n",
    "    units = units.lower()\n",
    "    num = float(num)\n",
    "    if units == 'k':\n",
    "        return str(num/10**3) + \" K\"\n",
    "    elif units == 'm':\n",
    "        return str(num/10**6) + \" M\"\n",
    "    elif units == 'b':\n",
    "        return str(num/10**9) +  \" B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt.title('Distribution of ratings over Training dataset', fontsize=15)\n",
    "sns.countplot(train_df.rating)\n",
    "ax.set_yticklabels([human(item, 'M') for item in ax.get_yticks()])\n",
    "ax.set_ylabel('No. of Ratings(Millions)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  \n",
    "\n",
    "train_df['day_of_week'] = train_df.date.dt.weekday_name\n",
    "\n",
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = train_df.resample('m', on='date')['rating'].count().plot()\n",
    "ax.set_title('No of ratings per month (Training data)')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('No of ratings(per month)')\n",
    "ax.set_yticklabels([human(item, 'M') for item in ax.get_yticks()])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_rated_movies_per_user = train_df.groupby(by='user')['rating'].count().sort_values(ascending=False)\n",
    "\n",
    "no_of_rated_movies_per_user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=plt.figaspect(.5))\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "sns.kdeplot(no_of_rated_movies_per_user, shade=True, ax=ax1)\n",
    "plt.xlabel('No of ratings by user')\n",
    "plt.title(\"PDF\")\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "sns.kdeplot(no_of_rated_movies_per_user, shade=True, cumulative=True,ax=ax2)\n",
    "plt.xlabel('No of ratings by user')\n",
    "plt.title('CDF')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_rated_movies_per_user.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Quantiles and their Values\")\n",
    "quantiles.plot()\n",
    "# quantiles with 0.05 difference\n",
    "plt.scatter(x=quantiles.index[::5], y=quantiles.values[::5], c='orange', label=\"quantiles with 0.05 intervals\")\n",
    "# quantiles with 0.25 difference\n",
    "plt.scatter(x=quantiles.index[::25], y=quantiles.values[::25], c='m', label = \"quantiles with 0.25 intervals\")\n",
    "plt.ylabel('No of ratings by user')\n",
    "plt.xlabel('Value at the quantile')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "# annotate the 25th, 50th, 75th and 100th percentile values....\n",
    "for x,y in zip(quantiles.index[::25], quantiles[::25]):\n",
    "    plt.annotate(s=\"({} , {})\".format(x,y), xy=(x,y), xytext=(x-0.05, y+500)\n",
    "                ,fontweight='bold')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles[::5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n No of ratings at last 5 percentile : {}\\n'.format(sum(no_of_rated_movies_per_user>= 749)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_ratings_per_movie = train_df.groupby(by='movie')['rating'].count().sort_values(ascending=False)\n",
    "\n",
    "fig = plt.figure(figsize=plt.figaspect(.5))\n",
    "ax = plt.gca()\n",
    "plt.plot(no_of_ratings_per_movie.values)\n",
    "plt.title('# RATINGS per Movie')\n",
    "plt.xlabel('Movie')\n",
    "plt.ylabel('No of Users who rated a movie')\n",
    "ax.set_xticklabels([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.countplot(x='day_of_week', data=train_df, ax=ax)\n",
    "plt.title('No of ratings on each day...')\n",
    "plt.ylabel('Total no of ratings')\n",
    "plt.xlabel('')\n",
    "ax.set_yticklabels([human(item, 'M') for item in ax.get_yticks()])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "fig = plt.figure(figsize=plt.figaspect(.45))\n",
    "sns.boxplot(y='rating', x='day_of_week', data=train_df)\n",
    "plt.show()\n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_week_df = train_df.groupby(by=['day_of_week'])['rating'].mean()\n",
    "print(\" AVerage ratings\")\n",
    "print(\"-\"*30)\n",
    "print(avg_week_df)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "if os.path.isfile('train_sparse_matrix.npz'):\n",
    "    print(\"It is present in your pwd, getting it from disk....\")\n",
    "    # just get it from the disk instead of computing it\n",
    "    train_sparse_matrix = sparse.load_npz('train_sparse_matrix.npz')\n",
    "    print(\"DONE..\")\n",
    "else: \n",
    "    print(\"We are creating sparse_matrix from the dataframe..\")\n",
    "    # create sparse_matrix and store it for after usage.\n",
    "    # csr_matrix(data_values, (row_index, col_index), shape_of_matrix)\n",
    "    # It should be in such a way that, MATRIX[row, col] = data\n",
    "    train_sparse_matrix = sparse.csr_matrix((train_df.rating.values, (train_df.user.values,\n",
    "                                               train_df.movie.values)),)\n",
    "    \n",
    "    print('Done. It\\'s shape is : (user, movie) : ',train_sparse_matrix.shape)\n",
    "    print('Saving it into disk for furthur usage..')\n",
    "    # save it into disk\n",
    "    sparse.save_npz(\"train_sparse_matrix.npz\", train_sparse_matrix)\n",
    "    print('Done..\\n')\n",
    "\n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us,mv = train_sparse_matrix.shape\n",
    "elem = train_sparse_matrix.count_nonzero()\n",
    "\n",
    "print(\"Sparsity Of Train matrix : {} % \".format(  (1-(elem/(us*mv))) * 100) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "if os.path.isfile('test_sparse_matrix.npz'):\n",
    "    print(\"It is present in your pwd, getting it from disk....\")\n",
    "    # just get it from the disk instead of computing it\n",
    "    test_sparse_matrix = sparse.load_npz('test_sparse_matrix.npz')\n",
    "    print(\"DONE..\")\n",
    "else: \n",
    "    print(\"We are creating sparse_matrix from the dataframe..\")\n",
    "    # create sparse_matrix and store it for after usage.\n",
    "    # csr_matrix(data_values, (row_index, col_index), shape_of_matrix)\n",
    "    # It should be in such a way that, MATRIX[row, col] = data\n",
    "    test_sparse_matrix = sparse.csr_matrix((test_df.rating.values, (test_df.user.values,\n",
    "                                               test_df.movie.values)))\n",
    "    \n",
    "    print('Done. It\\'s shape is : (user, movie) : ',test_sparse_matrix.shape)\n",
    "    print('Saving it into disk for furthur usage..')\n",
    "    # save it into disk\n",
    "    sparse.save_npz(\"test_sparse_matrix.npz\", test_sparse_matrix)\n",
    "    print('Done..\\n')\n",
    "    \n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us,mv = test_sparse_matrix.shape\n",
    "elem = test_sparse_matrix.count_nonzero()\n",
    "\n",
    "print(\"Sparsity Of Test matrix : {} % \".format(  (1-(elem/(us*mv))) * 100) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the user averages in dictionary (key: user_id/movie_id, value: avg rating)\n",
    "\n",
    "def get_average_ratings(sparse_matrix, of_users):\n",
    "    \n",
    "    # average ratings of user/axes\n",
    "    ax = 1 if of_users else 0 # 1 - User axes,0 - Movie axes\n",
    "\n",
    "    # \".A1\" is for converting Column_Matrix to 1-D numpy array \n",
    "    sum_of_ratings = sparse_matrix.sum(axis=ax).A1\n",
    "    # Boolean matrix of ratings ( whether a user rated that movie or not)\n",
    "    is_rated = sparse_matrix!=0\n",
    "    # no of ratings that each user OR movie..\n",
    "    no_of_ratings = is_rated.sum(axis=ax).A1\n",
    "    \n",
    "    # max_user  and max_movie ids in sparse matrix \n",
    "    u,m = sparse_matrix.shape\n",
    "    # creae a dictonary of users and their average ratigns..\n",
    "    average_ratings = { i : sum_of_ratings[i]/no_of_ratings[i]\n",
    "                                 for i in range(u if of_users else m) \n",
    "                                    if no_of_ratings[i] !=0}\n",
    "\n",
    "    # return that dictionary of average ratings\n",
    "    return average_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_averages = dict()\n",
    "# get the global average of ratings in our train set.\n",
    "train_global_average = train_sparse_matrix.sum()/train_sparse_matrix.count_nonzero()\n",
    "train_averages['global'] = train_global_average\n",
    "train_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_averages['user'] = get_average_ratings(train_sparse_matrix, of_users=True)\n",
    "print('\\nAverage rating of user 10 :',train_averages['user'][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_averages['movie'] =  get_average_ratings(train_sparse_matrix, of_users=False)\n",
    "print('\\n AVerage rating of movie 15 :',train_averages['movie'][15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "# draw pdfs for average rating per user and average\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=plt.figaspect(.5))\n",
    "fig.suptitle('Avg Ratings per User and per Movie', fontsize=15)\n",
    "\n",
    "ax1.set_title('Users-Avg-Ratings')\n",
    "# get the list of average user ratings from the averages dictionary..\n",
    "user_averages = [rat for rat in train_averages['user'].values()]\n",
    "sns.distplot(user_averages, ax=ax1, hist=False, \n",
    "             kde_kws=dict(cumulative=True), label='Cdf')\n",
    "sns.distplot(user_averages, ax=ax1, hist=False,label='Pdf')\n",
    "\n",
    "ax2.set_title('Movies-Avg-Rating')\n",
    "# get the list of movie_average_ratings from the dictionary..\n",
    "movie_averages = [rat for rat in train_averages['movie'].values()]\n",
    "sns.distplot(movie_averages, ax=ax2, hist=False, \n",
    "             kde_kws=dict(cumulative=True), label='Cdf')\n",
    "sns.distplot(movie_averages, ax=ax2, hist=False, label='Pdf')\n",
    "\n",
    "plt.show()\n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_users = len(np.unique(df.user))\n",
    "users_train = len(train_averages['user'])\n",
    "new_users = total_users - users_train\n",
    "\n",
    "print('\\nTotal number of Users  :', total_users)\n",
    "print('\\nNumber of Users in Train data :', users_train)\n",
    "print(\"\\nNo of Users that didn't appear in train data: {}({} %) \\n \".format(new_users,\n",
    "                                                                        np.round((new_users/total_users)*100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_movies = len(np.unique(df.movie))\n",
    "movies_train = len(train_averages['movie'])\n",
    "new_movies = total_movies - movies_train\n",
    "\n",
    "print('\\nTotal number of Movies  :', total_movies)\n",
    "print('\\nNumber of Users in Train data :', movies_train)\n",
    "print(\"\\nNo of Movies that didn't appear in train data: {}({} %) \\n \".format(new_movies,\n",
    "                                                                        np.round((new_movies/total_movies)*100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def compute_user_similarity(sparse_matrix, compute_for_few=False, top = 100, verbose=False, verb_for_n_rows = 20,\n",
    "                            draw_time_taken=True):\n",
    "    no_of_users, _ = sparse_matrix.shape\n",
    "    # get the indices of  non zero rows(users) from our sparse matrix\n",
    "    row_ind, col_ind = sparse_matrix.nonzero()\n",
    "    row_ind = sorted(set(row_ind)) # we don't have to\n",
    "    time_taken = list() #  time taken for finding similar users for an user..\n",
    "    \n",
    "    # we create rows, cols, and data lists.., which can be used to create sparse matrices\n",
    "    rows, cols, data = list(), list(), list()\n",
    "    if verbose: print(\"Computing top\",top,\"similarities for each user..\")\n",
    "    \n",
    "    start = datetime.now()\n",
    "    temp = 0\n",
    "    \n",
    "    for row in row_ind[:top] if compute_for_few else row_ind:\n",
    "        temp = temp+1\n",
    "        prev = datetime.now()\n",
    "        \n",
    "        # get the similarity row for this user with all other users\n",
    "        sim = cosine_similarity(sparse_matrix.getrow(row), sparse_matrix).ravel()\n",
    "        # We will get only the top ''top'' most similar users and ignore rest of them..\n",
    "        top_sim_ind = sim.argsort()[-top:]\n",
    "        top_sim_val = sim[top_sim_ind]\n",
    "        \n",
    "        # add them to our rows, cols and data\n",
    "        rows.extend([row]*top)\n",
    "        cols.extend(top_sim_ind)\n",
    "        data.extend(top_sim_val)\n",
    "        time_taken.append(datetime.now().timestamp() - prev.timestamp())\n",
    "        if verbose:\n",
    "            if temp%verb_for_n_rows == 0:\n",
    "                print(\"computing done for {} users [  time elapsed : {}  ]\"\n",
    "                      .format(temp, datetime.now()-start))\n",
    "            \n",
    "        \n",
    "    # lets create sparse matrix out of these and return it\n",
    "    if verbose: print('Creating Sparse matrix from the computed similarities')\n",
    "    #return rows, cols, data\n",
    "    \n",
    "    if draw_time_taken:\n",
    "        plt.plot(time_taken, label = 'time taken for each user')\n",
    "        plt.plot(np.cumsum(time_taken), label='Total time')\n",
    "        plt.legend(loc='best')\n",
    "        plt.xlabel('User')\n",
    "        plt.ylabel('Time (seconds)')\n",
    "        plt.show()\n",
    "        \n",
    "    return sparse.csr_matrix((data, (rows, cols)), shape=(no_of_users, no_of_users)), time_taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "u_u_sim_sparse, _ = compute_user_similarity(train_sparse_matrix, compute_for_few=True, top = 100,\n",
    "                                                     verbose=True)\n",
    "print(\"-\"*100)\n",
    "print(\"Time taken :\",datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "# initilaize the algorithm with some parameters..\n",
    "# All of them are default except n_components. n_itr is for Randomized SVD solver.\n",
    "netflix_svd = TruncatedSVD(n_components=500, algorithm='randomized', random_state=15)\n",
    "trunc_svd = netflix_svd.fit_transform(train_sparse_matrix)\n",
    "\n",
    "print(datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expl_var = np.cumsum(netflix_svd.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=plt.figaspect(.5))\n",
    "\n",
    "ax1.set_ylabel(\"Variance Explained\", fontsize=15)\n",
    "ax1.set_xlabel(\"# Latent Facors\", fontsize=15)\n",
    "ax1.plot(expl_var)\n",
    "# annote some (latentfactors, expl_var) to make it clear\n",
    "ind = [1, 2,4,8,20, 60, 100, 200, 300, 400, 500]\n",
    "ax1.scatter(x = [i-1 for i in ind], y = expl_var[[i-1 for i in ind]], c='#ff3300')\n",
    "for i in ind:\n",
    "    ax1.annotate(s =\"({}, {})\".format(i,  np.round(expl_var[i-1], 2)), xy=(i-1, expl_var[i-1]),\n",
    "                xytext = ( i+20, expl_var[i-1] - 0.01), fontweight='bold')\n",
    "\n",
    "change_in_expl_var = [expl_var[i+1] - expl_var[i] for i in range(len(expl_var)-1)]\n",
    "ax2.plot(change_in_expl_var)\n",
    "\n",
    "\n",
    "\n",
    "ax2.set_ylabel(\"Gain in Var_Expl with One Additional LF\", fontsize=10)\n",
    "ax2.yaxis.set_label_position(\"right\")\n",
    "ax2.set_xlabel(\"# Latent Facors\", fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ind:\n",
    "    print(\"({}, {})\".format(i, np.round(expl_var[i-1], 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's project our Original U_M matrix into into 500 Dimensional space...\n",
    "start = datetime.now()\n",
    "trunc_matrix = train_sparse_matrix.dot(netflix_svd.components_.T)\n",
    "print(datetime.now()- start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(trunc_matrix), trunc_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('trunc_sparse_matrix.npz'):\n",
    "    # create that sparse sparse matrix\n",
    "    trunc_sparse_matrix = sparse.csr_matrix(trunc_matrix)\n",
    "    # Save this truncated sparse matrix for later usage..\n",
    "    sparse.save_npz('trunc_sparse_matrix', trunc_sparse_matrix)\n",
    "else:\n",
    "    trunc_sparse_matrix = sparse.load_npz('trunc_sparse_matrix.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trunc_sparse_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "trunc_u_u_sim_matrix, _ = compute_user_similarity(trunc_sparse_matrix, compute_for_few=True, top=50, verbose=True, \n",
    "                                                 verb_for_n_rows=10)\n",
    "print(\"-\"*50)\n",
    "print(\"time:\",datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "if not os.path.isfile('m_m_sim_sparse.npz'):\n",
    "    print(\"It seems you don't have that file. Computing movie_movie similarity...\")\n",
    "    start = datetime.now()\n",
    "    m_m_sim_sparse = cosine_similarity(X=train_sparse_matrix.T, dense_output=False)\n",
    "    print(\"Done..\")\n",
    "    # store this sparse matrix in disk before using it. For future purposes.\n",
    "    print(\"Saving it to disk without the need of re-computing it again.. \")\n",
    "    sparse.save_npz(\"m_m_sim_sparse.npz\", m_m_sim_sparse)\n",
    "    print(\"Done..\")\n",
    "else:\n",
    "    print(\"It is there, We will get it.\")\n",
    "    m_m_sim_sparse = sparse.load_npz(\"m_m_sim_sparse.npz\")\n",
    "    print(\"Done ...\")\n",
    "\n",
    "print(\"It's a \",m_m_sim_sparse.shape,\" dimensional matrix\")\n",
    "\n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_m_sim_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_ids = np.unique(m_m_sim_sparse.nonzero()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "similar_movies = dict()\n",
    "for movie in movie_ids:\n",
    "    # get the top similar movies and store them in the dictionary\n",
    "    sim_movies = m_m_sim_sparse[movie].toarray().ravel().argsort()[::-1][1:]\n",
    "    similar_movies[movie] = sim_movies[:100]\n",
    "print(datetime.now() - start)\n",
    "\n",
    "# just testing similar movies for movie_15\n",
    "similar_movies[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Let's load the movie details into soe dataframe..\n",
    "# movie details are in 'netflix/movie_titles.csv'\n",
    "\n",
    "movie_titles = pd.read_csv(\"data_folder/movie_titles.csv\", sep=',', header = None,\n",
    "                           names=['movie_id', 'year_of_release', 'title'], verbose=True,\n",
    "                      index_col = 'movie_id', encoding = \"ISO-8859-1\")\n",
    "\n",
    "movie_titles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_id = 67\n",
    "\n",
    "print(\"\\nMovie ----->\",movie_titles.loc[mv_id].values[1])\n",
    "\n",
    "print(\"\\nIt has {} Ratings from users.\".format(train_sparse_matrix[:,mv_id].getnnz()))\n",
    "\n",
    "print(\"\\nWe have {} movies which are similarto this  and we will get only top most..\".format(m_m_sim_sparse[:,mv_id].getnnz()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = m_m_sim_sparse[mv_id].toarray().ravel()\n",
    "\n",
    "similar_indices = similarities.argsort()[::-1][1:]\n",
    "\n",
    "similarities[similar_indices]\n",
    "\n",
    "sim_indices = similarities.argsort()[::-1][1:] # It will sort and reverse the array and ignore its similarity (ie.,1)\n",
    "                                               # and return its indices(movie_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(similarities[sim_indices], label='All the ratings')\n",
    "plt.plot(similarities[sim_indices[:100]], label='top 100 similar movies')\n",
    "plt.title(\"Similar Movies of {}(movie_id)\".format(mv_id), fontsize=20)\n",
    "plt.xlabel(\"Movies (Not Movie_Ids)\", fontsize=15)\n",
    "plt.ylabel(\"Cosine Similarity\",fontsize=15)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_titles.loc[sim_indices[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_sparse_matrix(sparse_matrix, no_users, no_movies, path, verbose = True):\n",
    "    \"\"\"\n",
    "        It will get it from the ''path'' if it is present  or It will create \n",
    "        and store the sampled sparse matrix in the path specified.\n",
    "    \"\"\"\n",
    "\n",
    "    # get (row, col) and (rating) tuple from sparse_matrix...\n",
    "    row_ind, col_ind, ratings = sparse.find(sparse_matrix)\n",
    "    users = np.unique(row_ind)\n",
    "    movies = np.unique(col_ind)\n",
    "\n",
    "    print(\"Original Matrix : (users, movies) -- ({} {})\".format(len(users), len(movies)))\n",
    "    print(\"Original Matrix : Ratings -- {}\\n\".format(len(ratings)))\n",
    "\n",
    "    # It just to make sure to get same sample everytime we run this program..\n",
    "    # and pick without replacement....\n",
    "    np.random.seed(15)\n",
    "    sample_users = np.random.choice(users, no_users, replace=False)\n",
    "    sample_movies = np.random.choice(movies, no_movies, replace=False)\n",
    "    # get the boolean mask or these sampled_items in originl row/col_inds..\n",
    "    mask = np.logical_and( np.isin(row_ind, sample_users),\n",
    "                      np.isin(col_ind, sample_movies) )\n",
    "    \n",
    "    sample_sparse_matrix = sparse.csr_matrix((ratings[mask], (row_ind[mask], col_ind[mask])),\n",
    "                                             shape=(max(sample_users)+1, max(sample_movies)+1))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Sampled Matrix : (users, movies) -- ({} {})\".format(len(sample_users), len(sample_movies)))\n",
    "        print(\"Sampled Matrix : Ratings --\", format(ratings[mask].shape[0]))\n",
    "\n",
    "    print('Saving it into disk for furthur usage..')\n",
    "    # save it into disk\n",
    "    sparse.save_npz(path, sample_sparse_matrix)\n",
    "    if verbose:\n",
    "            print('Done..\\n')\n",
    "    \n",
    "    return sample_sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "path = \"sample/small/sample_train_sparse_matrix.npz\"\n",
    "if os.path.isfile(path):\n",
    "    print(\"It is present in your pwd, getting it from disk....\")\n",
    "    # just get it from the disk instead of computing it\n",
    "    sample_train_sparse_matrix = sparse.load_npz(path)\n",
    "    print(\"DONE..\")\n",
    "else: \n",
    "    # get 10k users and 1k movies from available data \n",
    "    sample_train_sparse_matrix = get_sample_sparse_matrix(train_sparse_matrix, no_users=10000, no_movies=1000,\n",
    "                                             path = path)\n",
    "\n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "path = \"sample/small/sample_test_sparse_matrix.npz\"\n",
    "if os.path.isfile(path):\n",
    "    print(\"It is present in your pwd, getting it from disk....\")\n",
    "    # just get it from the disk instead of computing it\n",
    "    sample_test_sparse_matrix = sparse.load_npz(path)\n",
    "    print(\"DONE..\")\n",
    "else:\n",
    "    # get 5k users and 500 movies from available data \n",
    "    sample_test_sparse_matrix = get_sample_sparse_matrix(test_sparse_matrix, no_users=5000, no_movies=500,\n",
    "                                                 path = \"sample/small/sample_test_sparse_matrix.npz\")\n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train_averages = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the global average of ratings in our train set.\n",
    "global_average = sample_train_sparse_matrix.sum()/sample_train_sparse_matrix.count_nonzero()\n",
    "sample_train_averages['global'] = global_average\n",
    "sample_train_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train_averages['user'] = get_average_ratings(sample_train_sparse_matrix, of_users=True)\n",
    "print('\\nAverage rating of user 1515220 :',sample_train_averages['user'][1515220])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train_averages['movie'] =  get_average_ratings(sample_train_sparse_matrix, of_users=False)\n",
    "print('\\n AVerage rating of movie 15153 :',sample_train_averages['movie'][15153])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n No of ratings in Our Sampled train matrix is : {}\\n'.format(sample_train_sparse_matrix.count_nonzero()))\n",
    "print('\\n No of ratings in Our Sampled test  matrix is : {}\\n'.format(sample_test_sparse_matrix.count_nonzero()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get users, movies and ratings from our samples train sparse matrix\n",
    "sample_train_users, sample_train_movies, sample_train_ratings = sparse.find(sample_train_sparse_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# It took me almost 10 hours to prepare this train dataset.#\n",
    "############################################################\n",
    "start = datetime.now()\n",
    "if os.path.isfile('sample/small/reg_train.csv'):\n",
    "    print(\"File already exists you don't have to prepare again...\" )\n",
    "else:\n",
    "    print('preparing {} tuples for the dataset..\\n'.format(len(sample_train_ratings)))\n",
    "    with open('sample/small/reg_train.csv', mode='w') as reg_data_file:\n",
    "        count = 0\n",
    "        for (user, movie, rating)  in zip(sample_train_users, sample_train_movies, sample_train_ratings):\n",
    "            st = datetime.now()\n",
    "        #     print(user, movie)    \n",
    "            #--------------------- Ratings of \"movie\" by similar users of \"user\" ---------------------\n",
    "            # compute the similar Users of the \"user\"        \n",
    "            user_sim = cosine_similarity(sample_train_sparse_matrix[user], sample_train_sparse_matrix).ravel()\n",
    "            top_sim_users = user_sim.argsort()[::-1][1:] # we are ignoring 'The User' from its similar users.\n",
    "            # get the ratings of most similar users for this movie\n",
    "            top_ratings = sample_train_sparse_matrix[top_sim_users, movie].toarray().ravel()\n",
    "            # we will make it's length \"5\" by adding movie averages to .\n",
    "            top_sim_users_ratings = list(top_ratings[top_ratings != 0][:5])\n",
    "            top_sim_users_ratings.extend([sample_train_averages['movie'][movie]]*(5 - len(top_sim_users_ratings)))\n",
    "        #     print(top_sim_users_ratings, end=\" \")    \n",
    "\n",
    "\n",
    "            #--------------------- Ratings by \"user\"  to similar movies of \"movie\" ---------------------\n",
    "            # compute the similar movies of the \"movie\"        \n",
    "            movie_sim = cosine_similarity(sample_train_sparse_matrix[:,movie].T, sample_train_sparse_matrix.T).ravel()\n",
    "            top_sim_movies = movie_sim.argsort()[::-1][1:] # we are ignoring 'The User' from its similar users.\n",
    "            # get the ratings of most similar movie rated by this user..\n",
    "            top_ratings = sample_train_sparse_matrix[user, top_sim_movies].toarray().ravel()\n",
    "            # we will make it's length \"5\" by adding user averages to.\n",
    "            top_sim_movies_ratings = list(top_ratings[top_ratings != 0][:5])\n",
    "            top_sim_movies_ratings.extend([sample_train_averages['user'][user]]*(5-len(top_sim_movies_ratings))) \n",
    "        #     print(top_sim_movies_ratings, end=\" : -- \")\n",
    "\n",
    "            #-----------------prepare the row to be stores in a file-----------------#\n",
    "            row = list()\n",
    "            row.append(user)\n",
    "            row.append(movie)\n",
    "            # Now add the other features to this data...\n",
    "            row.append(sample_train_averages['global']) # first feature\n",
    "            # next 5 features are similar_users \"movie\" ratings\n",
    "            row.extend(top_sim_users_ratings)\n",
    "            # next 5 features are \"user\" ratings for similar_movies\n",
    "            row.extend(top_sim_movies_ratings)\n",
    "            # Avg_user rating\n",
    "            row.append(sample_train_averages['user'][user])\n",
    "            # Avg_movie rating\n",
    "            row.append(sample_train_averages['movie'][movie])\n",
    "\n",
    "            # finalley, The actual Rating of this user-movie pair...\n",
    "            row.append(rating)\n",
    "            count = count + 1\n",
    "\n",
    "            # add rows to the file opened..\n",
    "            reg_data_file.write(','.join(map(str, row)))\n",
    "            reg_data_file.write('\\n')        \n",
    "            if (count)%10000 == 0:\n",
    "                # print(','.join(map(str, row)))\n",
    "                print(\"Done for {} rows----- {}\".format(count, datetime.now() - start))\n",
    "\n",
    "\n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_train = pd.read_csv('sample/small/reg_train.csv', names = ['user', 'movie', 'GAvg', 'sur1', 'sur2', 'sur3', 'sur4', 'sur5','smr1', 'smr2', 'smr3', 'smr4', 'smr5', 'UAvg', 'MAvg', 'rating'], header=None)\n",
    "reg_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get users, movies and ratings from the Sampled Test \n",
    "sample_test_users, sample_test_movies, sample_test_ratings = sparse.find(sample_test_sparse_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train_averages['global']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "if os.path.isfile('sample/small/reg_test.csv'):\n",
    "    print(\"It is already created...\")\n",
    "else:\n",
    "\n",
    "    print('preparing {} tuples for the dataset..\\n'.format(len(sample_test_ratings)))\n",
    "    with open('sample/small/reg_test.csv', mode='w') as reg_data_file:\n",
    "        count = 0 \n",
    "        for (user, movie, rating)  in zip(sample_test_users, sample_test_movies, sample_test_ratings):\n",
    "            st = datetime.now()\n",
    "\n",
    "        #--------------------- Ratings of \"movie\" by similar users of \"user\" ---------------------\n",
    "            #print(user, movie)\n",
    "            try:\n",
    "                # compute the similar Users of the \"user\"        \n",
    "                user_sim = cosine_similarity(sample_train_sparse_matrix[user], sample_train_sparse_matrix).ravel()\n",
    "                top_sim_users = user_sim.argsort()[::-1][1:] # we are ignoring 'The User' from its similar users.\n",
    "                # get the ratings of most similar users for this movie\n",
    "                top_ratings = sample_train_sparse_matrix[top_sim_users, movie].toarray().ravel()\n",
    "                # we will make it's length \"5\" by adding movie averages to .\n",
    "                top_sim_users_ratings = list(top_ratings[top_ratings != 0][:5])\n",
    "                top_sim_users_ratings.extend([sample_train_averages['movie'][movie]]*(5 - len(top_sim_users_ratings)))\n",
    "                # print(top_sim_users_ratings, end=\"--\")\n",
    "\n",
    "            except (IndexError, KeyError):\n",
    "                # It is a new User or new Movie or there are no ratings for given user for top similar movies...\n",
    "                ########## Cold STart Problem ##########\n",
    "                top_sim_users_ratings.extend([sample_train_averages['global']]*(5 - len(top_sim_users_ratings)))\n",
    "                #print(top_sim_users_ratings)\n",
    "            except:\n",
    "                print(user, movie)\n",
    "                # we just want KeyErrors to be resolved. Not every Exception...\n",
    "                raise\n",
    "\n",
    "\n",
    "\n",
    "            #--------------------- Ratings by \"user\"  to similar movies of \"movie\" ---------------------\n",
    "            try:\n",
    "                # compute the similar movies of the \"movie\"        \n",
    "                movie_sim = cosine_similarity(sample_train_sparse_matrix[:,movie].T, sample_train_sparse_matrix.T).ravel()\n",
    "                top_sim_movies = movie_sim.argsort()[::-1][1:] # we are ignoring 'The User' from its similar users.\n",
    "                # get the ratings of most similar movie rated by this user..\n",
    "                top_ratings = sample_train_sparse_matrix[user, top_sim_movies].toarray().ravel()\n",
    "                # we will make it's length \"5\" by adding user averages to.\n",
    "                top_sim_movies_ratings = list(top_ratings[top_ratings != 0][:5])\n",
    "                top_sim_movies_ratings.extend([sample_train_averages['user'][user]]*(5-len(top_sim_movies_ratings))) \n",
    "                #print(top_sim_movies_ratings)\n",
    "            except (IndexError, KeyError):\n",
    "                #print(top_sim_movies_ratings, end=\" : -- \")\n",
    "                top_sim_movies_ratings.extend([sample_train_averages['global']]*(5-len(top_sim_movies_ratings)))\n",
    "                #print(top_sim_movies_ratings)\n",
    "            except :\n",
    "                raise\n",
    "\n",
    "            #-----------------prepare the row to be stores in a file-----------------#\n",
    "            row = list()\n",
    "            # add usser and movie name first\n",
    "            row.append(user)\n",
    "            row.append(movie)\n",
    "            row.append(sample_train_averages['global']) # first feature\n",
    "            #print(row)\n",
    "            # next 5 features are similar_users \"movie\" ratings\n",
    "            row.extend(top_sim_users_ratings)\n",
    "            #print(row)\n",
    "            # next 5 features are \"user\" ratings for similar_movies\n",
    "            row.extend(top_sim_movies_ratings)\n",
    "            #print(row)\n",
    "            # Avg_user rating\n",
    "            try:\n",
    "                row.append(sample_train_averages['user'][user])\n",
    "            except KeyError:\n",
    "                row.append(sample_train_averages['global'])\n",
    "            except:\n",
    "                raise\n",
    "            #print(row)\n",
    "            # Avg_movie rating\n",
    "            try:\n",
    "                row.append(sample_train_averages['movie'][movie])\n",
    "            except KeyError:\n",
    "                row.append(sample_train_averages['global'])\n",
    "            except:\n",
    "                raise\n",
    "            #print(row)\n",
    "            # finalley, The actual Rating of this user-movie pair...\n",
    "            row.append(rating)\n",
    "            #print(row)\n",
    "            count = count + 1\n",
    "\n",
    "            # add rows to the file opened..\n",
    "            reg_data_file.write(','.join(map(str, row)))\n",
    "            #print(','.join(map(str, row)))\n",
    "            reg_data_file.write('\\n')        \n",
    "            if (count)%1000 == 0:\n",
    "                #print(','.join(map(str, row)))\n",
    "                print(\"Done for {} rows----- {}\".format(count, datetime.now() - start))\n",
    "    print(\"\",datetime.now() - start)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_test_df = pd.read_csv('sample/small/reg_test.csv', names = ['user', 'movie', 'GAvg', 'sur1', 'sur2', 'sur3', 'sur4', 'sur5',\n",
    "                                                          'smr1', 'smr2', 'smr3', 'smr4', 'smr5',\n",
    "                                                          'UAvg', 'MAvg', 'rating'], header=None)\n",
    "reg_test_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Reader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Reader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-e3cb64c00fd1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# It is to specify how to read the dataframe.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# for our dataframe, we don't have to specify anything extra..\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrating_scale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# create the traindata from the dataframe...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Reader' is not defined"
     ]
    }
   ],
   "source": [
    "# It is to specify how to read the dataframe.\n",
    "# for our dataframe, we don't have to specify anything extra..\n",
    "reader = Reader(rating_scale=(1,5))\n",
    "\n",
    "# create the traindata from the dataframe...\n",
    "train_data = Dataset.load_from_df(reg_train[['user', 'movie', 'rating']], reader)\n",
    "\n",
    "# build the trainset from traindata.., It is of dataset format from surprise library..\n",
    "trainset = train_data.build_full_trainset() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
